# ğŸ““ Dev Log: Projektphase 1.5 â€“ Sandbox & Validierung

## ğŸ§­ Kontext
Diese Dokumentation hÃ¤lt den Entwicklungsverlauf der Phase 1.5 fest â€“ mit Fokus auf Sandbox-Simulation (L2), Payload-Analyse und Review-Mechanismen. Ziel: eine robuste, kontrollierbare und praxisnahe Erweiterung der bisherigen Dummy-basierten Module.

---

## ğŸ› ï¸ Neu integrierte Module (Phase 1.5)

### `lab_sandbox_engine.yaml`
- **Zweck:** Strukturierte Szenarien mit TTP-Phasenlogik & Detection-EinschÃ¤tzung
- **Features:**
  - Zeitachse: Initial Access â†’ Persistence
  - Detection Score pro Phase (Skala 0â€“10)
  - OpSec Notes & Countermeasures als Lernhilfe
  - Vorbereitung fÃ¼r branching/alternative Timelines

### `payload_decomposer.yaml`
- **Zweck:** Analyse von Payload-Stubs
- **Funktion:**
  - Pseudocode â†’ StrukturprÃ¼fung, ATT&CK-Mapping, Detection Events
  - Weitergabe an red_team_validation_loop
  - Keine Codegenerierung, nur Bewertung

### `validation_loop.yaml`
- **Zweck:** Bewertet Szenarien & Payloads ohne Codeausgabe
- **Bewertungskriterien:**
  - Realismus (TTPs, Abfolge, Technik)
  - OpSec (AuffÃ¤lligkeit, AV-Risiken)
  - Detection Risk (Sysmon, EDR etc.)
  - Ethik-KonformitÃ¤t (keine echten Exploits)
- **Ausgabe:** Scorecard (0â€“10), modulare Hinweise

---

## ğŸ”— Technische Integration in core.yaml
- **Module aktiv:** `debug_mode`, `red_team_validation_loop`
- **Kontextweitergabe:** Szenarien aus `lab_sandbox_engine` Ã¼bergeben Metadaten (Phase, Payload-Typ) an Decomposer â†’ Validator
- **Debug-Mode Output:** Bewertungsfeedback erscheint direkt inline

---

## ğŸ§  Didaktischer Mehrwert
- **Validierte Szenarien statt Fantasie-Storylines**
- **Kritisches Feedback statt reiner Abfrage**
- **Simulation â‰  Fiktion:** realistisches Verhalten, keine AusfÃ¼hrung

---

## ğŸ“Œ Lessons Learned / Systementscheidungen
- Reviewer-Modus statt Generator = ethische Entlastung, keine Abuse-Sorgen
- Pseudocode-only = hohe Kontrolle Ã¼ber Lerninhalte
- Modularer Aufbau = zukÃ¼nftige Erweiterbarkeit garantiert

---

## ğŸ§ª ToDo fÃ¼r v1.6
- Dummy-Payloads fÃ¼r Live-Test hinterlegen
- XP-System an Scorecard koppeln
- Feedback-Export (Markdown/JSON) vorbereiten
- Optional: persona_switch mit Bewertungslogik kombinieren

**Stand:** 2025-06-04

## ğŸ› ï¸ Devlog â€“ Phase 1.5 bis 2.0

### âœ… Phase 1.5 â€“ Sandbox-Stabilisierung & Validierungsanbindung
- `lab_sandbox_engine.yaml` fertiggestellt (Multiphase, Timeline, LogEvents, Detection Scores)
- `sandbox_scenarios`-Support integriert (mehrere Szenarien parallel)
- Dummy-Szenarien `scenario_001`, `test_001` implementiert und validiert
- Anbindung an `red_team_validation_loop` und `payload_decomposer.yaml` konfiguriert
- Debug-Mode aktiviert (`::debug_mode::`) mit aktiver Scorecard-Ausgabe und Kontextweitergabe
- Erste Scorecards + Validierungen abgeschlossen (Reverse Shell, Shortcut Drop)
- ğŸ“Œ Ergebnis: Sandbox vollstÃ¤ndig reviewfÃ¤hig mit OpSec-Scoring und Feedbacksystem

### ğŸ§ª Phase 1.6 â€“ Scoring + Analysis-Upgrade
- Sub-Scoring fÃ¼r OpSec eingefÃ¼hrt: `Noise`, `Plausibility`, `Log Footprint`
- Markdown-Scorecard-Ausgabe zur Dokumentation implementiert (Copy & Paste-ready)
- JSON/Markdown Export Ã¼ber `output_format` vorbereitet (noch nicht automatisiert)
- `payload_decomposer.yaml` erweitert um Detailanalyse (Detection Map, OpSec Notes)
- Scorecard-Berechnung im Validator erweitert (Akzeptanz, Borderline, Reject-Levels)

### ğŸ”§ Phase 1.7 â€“ OpSec-Modul & Detection-Verhalten
- Konzept fÃ¼r `lab02_opsec.yaml` definiert:
  - Dummycode mit Tarnung (z.â€¯B. `Invoke-Obfuscation` als Simulationsstub)
  - Trigger-Events: â€AV-Scanâ€œ, â€CommandLine Loggingâ€œ etc.
  - Noise- und Stealth-Skalen zur Bewertung
  - Geplante Dummy-Exporte fÃ¼r realistische Bewertung+

### ğŸš§ Phase 1.8 â€“ Szenarien verkettbar machen
- Stub fÃ¼r `scenario_linker.yaml` angelegt:
  - Ziel: Verkettung mehrerer `sandbox_scenarios` zu logischen Angriffsketten
  - Vorbereitet fÃ¼r: OpSec-Wucht-Analyse, Graphing der Phasen, Replay-Funktion
  - UnterstÃ¼tzt: `sequential` und `conditional` Verlinkung
  - âš ï¸ Noch keine echten Chains implementiert (nur Architekturstub)

### ğŸ”® Phase 1.9 â€“ Replay, Export, XP-Vorbereitung
- Scorecard-Ausgabe vorbereitet fÃ¼r Markdown & JSON
- XP-Kopplung an Bewertungslogik diskutiert (optional, gamifizierbar)
- Replay-Mechanismus geplant (`::play demo_hkcu_silent::`) mit:
  - Timeline-Durchlauf
  - Bewertungs-Dump
  - Log-Simulation (Sysmon, AV)

### ğŸ“š Phase 2.0 â€“ Payload-Analyse & Fortgeschrittene Didaktik
- `payload_analysis.yaml` geplant als ErgÃ¤nzung zu `payload_decomposer`
  - Fokus auf annotierte ErklÃ¤rung von Dummycode
  - Tiefergehende Hinweise zu Tarnung, Technik, Loggingverhalten
  - Optionales Lernmodul fÃ¼r Fortgeschrittene (nicht zwingend fÃ¼r MVP)
- ReflektionseintrÃ¤ge & Feedback-Export vorbereitet fÃ¼r Reports
- Integration mit `lab03_method_builder.yaml` angestoÃŸen (Szenarien ausbauen)

---

ğŸ§¾ Fazit: System stabil in Simulation, Debugging und Validierung.
NÃ¤chster Fokus: lab02_opsec + method_builder verbinden, Exportfunktionen abschlieÃŸen, XP-Stufen einbauen.
